{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Training and Mapping\n",
    "\n",
    "This notebook trains machine learning models (currently random forest classifiers) on several kinds of data to create flood hazard maps.\n",
    "\n",
    "The algorithm used in this notebooks is as follows:\n",
    "1. Load bands from various satellites and derive relevant information from them (e.g. water indicies like MNDWI). This includes elevation data from [SRTM](https://www2.jpl.nasa.gov/srtm/) and precipitation data from CHIRPS ([info here](https://www.chc.ucsb.edu/data/chirps), [data here](https://data.chc.ucsb.edu/products/CHIRPS-2.0/)).\n",
    "2. Calculate the \"summary statistics\" for each data variable across time (e.g. water index for a particular satellite). These statistics can be min, mean, max, and standard deviation - or just mean for binary variables. This results in a collection of 2D datasets (spatial dimensions - \"composites\" in a sense).\n",
    "3. Run a [linear discriminant analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) to identify the summary stats that best help predict flood hazard areas. This can provide insight into which summary statistics are the most helpful in identifying flood hazard areas, including the degree of hazard. **However, the LDA analysis only indicates how well the input data (features) allows the output data (given hazard map) to be classified. It does not indicate how well a model trained on this data may generalize to unseen inputs.**\n",
    "4. Train the machine learning models.\n",
    "5. Create flood hazard maps with the trained models.\n",
    "\n",
    "The machine learning models are created in the **models** directory (top level of the repository, not within this notebook's directory - same applies for other output directories). The models are named **hazard_{cross validation score}_{model parameter set}.joblib**. You can find code to load them in this notebook (**joblib.load()**).\n",
    "\n",
    "The hazard maps for Dakar are created in the **outputs** directory. The images are named **hazard_{cross validation score}_{model parameter set}.png**. Models with higher scores should generalize better than ones with lower scores. Comparing these images to the given flood hazard map (EO4SD) should provide an indication of the best the model will perform if the given flood hazard map provides sufficient data for the machine learning model to generalize to the coastline of Senegal (on whatever data the model will be run).\n",
    "\n",
    "The hazard maps for the coastline of Senegal are created in the **hazard_maps** directory in subdirectories named for the area (see the `areas` variable in the **Create the maps** section to see and change the areas). The images are named **{cross validation score}_{model parameter set}.png**. Models with higher scores should generalize better than ones with lower scores (so areas other than the Dakar training area should have better outputs if the models input features and training data are suitable for generalization). **Comparing these images to the given flood hazard map (EO4SD) should provide an indication of how well the models will perform if the given flood hazard map provides sufficient data for the machine learning model to generalize to the coastline of Senegal (on whatever data the model will be run).**\n",
    "\n",
    "# Index\n",
    "\n",
    "* Import dependencies, setup Dask client, and connect to the data cube\n",
    "* Load flood hazard data from World Bank\n",
    "* Show area to load data for\n",
    "* Load geospatial data\n",
    "    * Sentinel-2\n",
    "* Load elevation data (SRTM)\n",
    "* Load precipitation data from CHIRPS\n",
    "* Combine datasets\n",
    "* Train a flood risk classifier\n",
    "* Create the maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies, setup Dask client, and connect to the data cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import ChainMap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.ceos_utils.dc_display_map import display_map\n",
    "from utils.deafrica_utils.deafrica_bandindices import \\\n",
    "    calculate_indices\n",
    "from utils.deafrica_utils.deafrica_datahandling import load_ard\n",
    "\n",
    "import datacube\n",
    "dc = datacube.Datacube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ceos_utils.dask import create_local_dask_cluster\n",
    "\n",
    "client = create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load flood hazard data from World Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dakar_flood_hazard = gpd.read_file('../floodareas/eo4sd_dakar_fhazard_2018/EO4SD_DAKAR_FHAZARD_2018.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove records with no geometry data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dakar_flood_hazard = dakar_flood_hazard[[dakar_flood_hazard.geometry[i] is not None for i in range(len(dakar_flood_hazard))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change the CRS to EPSG:4326**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dakar_flood_hazard = dakar_flood_hazard.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the bounding box of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dakar_bounds = dakar_flood_hazard.bounds\n",
    "min_lon = dakar_bounds.minx.min()\n",
    "max_lon = dakar_bounds.maxx.max()\n",
    "min_lat = dakar_bounds.miny.min()\n",
    "max_lat = dakar_bounds.maxy.max()\n",
    "lat = (min_lat, max_lat)\n",
    "lon = (min_lon, max_lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show area to load data for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dakar, Senegal\n",
    "# Small test\n",
    "# lat = (14.8270, 14.8422)\n",
    "# lon = (-17.2576, -17.2172)\n",
    "# Tip\n",
    "# lat = (14.6433, 14.7892)\n",
    "# lon = (-17.5408, -17.4158)\n",
    "# Full\n",
    "lat = (14.6285, 14.8725)\n",
    "lon = (-17.5348, -17.2068)\n",
    "\n",
    "## Coast of Sengal\n",
    "# North\n",
    "# lat = (14.3559, 16.0974)\n",
    "# lon = (-17.5683, -16.4543)\n",
    "# Full\n",
    "# lat = (12.3016, 16.1810)\n",
    "# lon = (-17.8198, -16.3257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_map(lat, lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load geospatial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify time range and common load parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2013, 2020) # (inclusive, exclusive)\n",
    "time_ranges = [(f\"{year}-01-01\", f\"{year}-12-31\") for year in years]\n",
    "common_load_params = \\\n",
    "    dict(output_crs=\"EPSG:4326\",\n",
    "         resolution=(-0.00027,0.00027),\n",
    "         latitude=lat, longitude=lon,\n",
    "         dask_chunks={'time':40, \n",
    "                      'latitude':1000, \n",
    "                      'longitude':1000})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### WOfS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.ceos_utils.dc_load import is_dataset_empty\n",
    "\n",
    "# ls_data = []\n",
    "# for time_range in time_ranges:\n",
    "#     data = dc.load(product='ga_ls8c_wofs_2', \n",
    "#                    measurements=['water'], \n",
    "#                    time=time_range,\n",
    "#                    **common_load_params)\n",
    "#     if not is_dataset_empty(data):\n",
    "        \n",
    "#         # Formatting water data #\n",
    "#         # bit 7 indicates water, bit 2 indicates sea.\n",
    "#         ls_water_cls = (data.water&0b10000010)!=0\n",
    "#         # Set no_data (missing) values to NaN.\n",
    "#         ls_water_cls = \\\n",
    "#             ls_water_cls.where(data.water!=1)\n",
    "#         data['water'] = ls_water_cls\n",
    "#         # End formatting water data #\n",
    "        \n",
    "#         ls_data.append(data.rename({'water':'wofs'}))\n",
    "# ls_data = xr.concat(ls_data, dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls_data = dc.load(product='ga_ls8c_wofs_2', \n",
    "#                   measurements=['water'], \n",
    "#                   time=full_time_range,\n",
    "#                   **common_load_params)\n",
    "# ls_data = ls_data.sel(time=[list(time_range) for time_range in time_ranges])\n",
    "# # ls_data = xr.merge((ls_data_red, ls_data_water))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rename data variables to distinguish them from those of other datasets when we merge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls_data = ls_data.rename({data_var: f\"ls_{data_var}\" for data_var in ls_data.data_vars})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate summary statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls_stats = [{\n",
    "#              f'ls_wofs_mean':  ls_data.ls_wofs.mean('time') \n",
    "#            }]\n",
    "# ls_stats = xr.Dataset(dict(ChainMap(*ls_stats)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Impute remaining NaNs with the means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data_var in ls_stats.data_vars:\n",
    "#     ls_stats[data_var] = ls_stats[data_var]\\\n",
    "#         .where(~np.isnan(ls_stats[data_var]), ls_stats[data_var].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### Sentinel-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_data = []\n",
    "for time_range in time_ranges:\n",
    "    try:\n",
    "        data = load_ard(dc, products=['s2_l2a'],\n",
    "                        measurements=[\n",
    "                            # Used by MNDWI, AWEI_ns, AWEI_sh\n",
    "                            'green', 'swir_1', \n",
    "                            # Used by AWEI_ns, AWEI_sh\n",
    "                            'nir', 'swir_2',\n",
    "                            # Used by AWEI_sh\n",
    "                            'blue',\n",
    "                            'AOT', 'SCL'], \n",
    "                        time=time_range,\n",
    "                        **common_load_params).persist() # This will likely require a lot of RAM or storage.\n",
    "        data = calculate_indices(data, index='MNDWI', collection='s2')\n",
    "        data = calculate_indices(data, index='AWEI_ns', collection='s2')\n",
    "        data = calculate_indices(data, index='AWEI_sh', collection='s2')\n",
    "        data = data[['MNDWI', 'AWEI_ns', 'AWEI_sh', 'SCL', 'AOT']]\n",
    "        s2_data.append(data)\n",
    "    except:\n",
    "        continue\n",
    "s2_data = xr.concat(s2_data, dim='time')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rename data variables to distinguish them from those of other datasets when we merge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_data = s2_data.rename({data_var: f\"s2_{data_var}\" for data_var in s2_data.data_vars})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate mean of bare soil, water, and bare soil to water transitions across time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_bare_soil = s2_data.s2_SCL == 5\n",
    "s2_water = s2_data.s2_SCL == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_bare_soil_mean = s2_bare_soil.mean('time')\n",
    "s2_water_mean = s2_water.mean('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_soil_to_water = s2_water.isel(time=slice(1, len(s2_data.time))) & \\\n",
    "                   s2_bare_soil.isel(time=slice(0, len(s2_data.time)-1)).data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_soil_to_water_mean = s2_soil_to_water.mean('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate summary statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "s2_stats = [{\n",
    "             f'{data_var}_min':   s2_data[data_var].min('time'), \n",
    "             f'{data_var}_mean':  s2_data[data_var].mean('time'), \n",
    "             f'{data_var}_std':   s2_data[data_var].std('time'), \n",
    "             f'{data_var}_max':   s2_data[data_var].max('time')\n",
    "            }\n",
    "            for data_var in s2_data.data_vars if data_var != 's2_SCL'] + \\\n",
    "           [{\n",
    "             # The most common classification for each pixel.\n",
    "#              's2_SCL_mode': xr.DataArray(mode(s2_data.s2_SCL, axis=s2_data.s2_SCL.get_axis_num('time'))[0].squeeze(), \n",
    "#              coords={'latitude':s2_data.latitude, 'longitude':s2_data.longitude}, \n",
    "#              dims=['latitude', 'longitude']),\n",
    "             's2_soil_to_water_mean': s2_soil_to_water_mean, \n",
    "             's2_bare_soil_mean': s2_bare_soil_mean,\n",
    "             's2_water_mean': s2_water_mean,\n",
    "            }]\n",
    "\n",
    "s2_stats = xr.Dataset(dict(ChainMap(*s2_stats)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Impute remaining NaNs with the means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_var in s2_stats.data_vars:\n",
    "    s2_stats[data_var] = s2_stats[data_var]\\\n",
    "        .where(~np.isnan(s2_stats[data_var]), s2_stats[data_var].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load elevation data (SRTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 1 time, so we remove it with `.isel(time=0)`.\n",
    "srtm_data = \\\n",
    "    dc.load(product='srtm', \n",
    "            **common_load_params).elevation.isel(time=0)\n",
    "# Remove no_data values.\n",
    "srtm_data = srtm_data.where(srtm_data!=-32768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Impute remaining NaNs with the means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srtm_data = srtm_data.where(~np.isnan(srtm_data), srtm_data.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load precipitation data from CHIRPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "chirps_months = list(map(lambda month_str: month_str.zfill(2), map(str, range(1, 13))))\n",
    "chirps_data = xr.concat([xr.open_rasterio(f'../precipitation/chirps/chirps-v2.0.{year}.{month_str}.tif').squeeze()\n",
    "                         .sel(y=slice(*lat[::-1]), x=slice(*lon)) \n",
    "                         for year, month_str in itertools.product(years, chirps_months)], \n",
    "                        dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load and show CHIRPS data for the coast of Senegal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chirps_months = map(lambda month_str: month_str.zfill(2), map(str, range(1, 13)))\n",
    "# coast_lat = (12.3016, 16.1810)\n",
    "# coast_lon = (-17.8198, -16.3257)\n",
    "# chirps_data_coast = xr.concat([xr.open_rasterio(f'../precipitation/chirps/chirps-v2.0.{year}.{month_str}.tif').squeeze() \n",
    "#                                .sel(y=slice(*coast_lat[::-1]), x=slice(*coast_lon)) \n",
    "#                          for year, month_str in itertools.product(years, chirps_months)], \n",
    "#                         dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chirps_data_coast = chirps_data_coast.where(chirps_data_coast != -9999).rename({'x': 'longitude', 'y':'latitude'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(3,8))\n",
    "# chirps_data_coast.rename('mm / month').mean('time').plot.imshow(vmin=0, vmax=150)\n",
    "# plt.title('CHIRPS Precipitation')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set missing data points to NaN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chirps_data = chirps_data.where(chirps_data != -9999).rename({'x': 'longitude', 'y':'latitude'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate summary statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chirps_stats = [{\n",
    "             f'chirps_min':   chirps_data.min('time'), \n",
    "             f'chirps_mean':  chirps_data.mean('time'), \n",
    "             f'chirps_std':   chirps_data.std('time'), \n",
    "             f'chirps_max':   chirps_data.max('time')\n",
    "            }]\n",
    "chirps_stats = xr.Dataset(dict(ChainMap(*chirps_stats)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Impute remaining NaNs with the means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_var in chirps_stats.data_vars:\n",
    "    chirps_stats[data_var] = chirps_stats[data_var]\\\n",
    "        .where(~np.isnan(chirps_stats[data_var]), chirps_stats[data_var].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the CHIRPS data to the Landsat and Sentinel-2 data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chirps_stats = chirps_stats.reindex(latitude=s2_stats.latitude, longitude=s2_stats.longitude, method='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_stats = xr.merge((s2_stats, srtm_data, chirps_stats), join='left').persist()\n",
    "output_shape = merged_stats.s2_MNDWI_mean.shape\n",
    "output_coords = merged_stats.coords\n",
    "output_dims = merged_stats.dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a flood risk classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get an encoding for the flood risk classes and a raster mask of the flood risk areas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out areas that are commonly water from the risk maps\n",
    "# `dakar_flood_hazard['RISKCODE_H']==1` is very wrong without this.\n",
    "s2_land_mask = merged_stats.s2_MNDWI_mean < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.deafrica_utils.deafrica_spatialtools import xr_rasterize\n",
    "\n",
    "flood_hazard_enc = {'0':0, 'Low Risk':1, 'Medium Risk':2, 'High Risk':3}\n",
    "flood_hazard_masks = \\\n",
    "{'0': xr_rasterize(dakar_flood_hazard[dakar_flood_hazard['RISKCODE_H']==0], \n",
    "                   merged_stats).astype(np.bool).where(s2_land_mask, 0),\n",
    " 'Low Risk': xr_rasterize(dakar_flood_hazard[dakar_flood_hazard['RISKCODE_H']==1], \n",
    "                          merged_stats).astype(np.bool).where(s2_land_mask, 0),\n",
    " 'Medium Risk': xr_rasterize(dakar_flood_hazard[dakar_flood_hazard['RISKCODE_H']==2], \n",
    "                             merged_stats).astype(np.bool).where(s2_land_mask, 0),\n",
    " 'High Risk': xr_rasterize(dakar_flood_hazard[dakar_flood_hazard['RISKCODE_H']==3], \n",
    "                           merged_stats).astype(np.bool).where(s2_land_mask, 0)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert values to labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_hazard_enc_rev = {v: k for k, v in flood_hazard_enc.items()}\n",
    "dakar_flood_hazard_enc = dakar_flood_hazard.copy()\n",
    "dakar_flood_hazard_enc['RISKCODE_H'] = dakar_flood_hazard['RISKCODE_H'].map(flood_hazard_enc_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that the border of low risk flooding around the coast is removed in processing later**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "dakar_flood_hazard_enc[dakar_flood_hazard_enc['RISKCODE_H']!='0']\\\n",
    "    .plot(column='RISKCODE_H', legend=True, ax=ax)\n",
    "plt.title('Flood Hazard')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Format the feature matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_stats.to_array().transpose('latitude', 'longitude', 'variable')\n",
    "X = X.stack(row=('latitude', 'longitude')).transpose('row', 'variable').persist()\n",
    "X_local = X.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Format the truth matrix (risk classifications)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X.isel(variable=0)\n",
    "y = y.where(False, 0)\n",
    "for key, mask in flood_hazard_masks.items():\n",
    "    mask = mask.stack(row=('latitude', 'longitude'))\n",
    "    y = y.where(~mask.astype(bool), flood_hazard_enc[key])\n",
    "y = y.persist()\n",
    "y_local = y.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove unneeded data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the persisted data in `merged_stats` to save memory.\n",
    "if 'merged_stats' in globals():\n",
    "    del merged_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determine the relative importance of the data variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda = lda.fit(X_local, y_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the relative frequency of the classes to use as weights for the LDA coefficients.\n",
    "_, cls_frq_wgts = np.unique(y, return_counts=True)\n",
    "cls_frq_wgts = cls_frq_wgts / cls_frq_wgts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight the LDA coefficients by class frequency.\n",
    "lda_coef = (cls_frq_wgts*lda.coef_.T).T.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LDA coefficients for the features in descending order.\n",
    "desc_lda_coef_inds = np.argsort(abs(lda_coef))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_table = pd.DataFrame({'name': X['variable'].values, 'coef': lda_coef})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data variable names by the absolute value of the sum of their coefficients.\n",
    "lda_table['abs_coef'] = abs(lda_table.coef)\n",
    "lda_table = lda_table.sort_values('abs_coef', ascending=False)\n",
    "lda_table = lda_table.set_index('name')\n",
    "lda_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_table.abs_coef.plot()\n",
    "plt.xticks(ticks=range(len(lda_table)), \n",
    "           labels=lda_table.index.values, \n",
    "           rotation=70, ha='right')\n",
    "plt.title('LDA importance of inputs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train and save a classifier for each parameter set in a parameter grid and output the predictions as an image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vis = xr.DataArray(data=y.values.reshape(output_shape), \n",
    "                     coords=output_coords, dims=output_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vis.plot.imshow(vmin=0, vmax=3, figsize=(12,7))\n",
    "plt.title('Input Classifications (Given Flood Hazard Map)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"><b>Set the parameter grid here</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pathlib import Path\n",
    "from joblib import dump, load\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [16],\n",
    "    'max_depth': [24],\n",
    "    'min_samples_split': [2, 5, 15, 25, 50],\n",
    "    'min_samples_leaf': [2, 5, 15, 25, 50],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from functools import partial, update_wrapper\n",
    "\n",
    "def wrapped_partial(func, *args, **kwargs):\n",
    "    partial_func = partial(func, *args, **kwargs)\n",
    "    update_wrapper(partial_func, func)\n",
    "    return partial_func\n",
    "\n",
    "clf = RandomForestClassifier(class_weight='balanced', n_jobs=8)\n",
    "cv = StratifiedShuffleSplit(n_splits=5)\n",
    "scorer = make_scorer(wrapped_partial(f1_score, average='micro'))\n",
    "\n",
    "param_set_keys = list(param_grid.keys())\n",
    "param_sets_vals = list(itertools.product(*param_grid.values()))\n",
    "for i, param_set_vals in enumerate(param_sets_vals):\n",
    "    # Train\n",
    "    param_set = {k:v for k, v in zip(param_set_keys, param_set_vals)}\n",
    "    print(f\"Training with param set {param_set}\")\n",
    "    clf.set_params(**param_set)\n",
    "    # Using GridSearchCV on empty param grid just to use `cv` conveniently.\n",
    "    grid_search = \\\n",
    "        GridSearchCV(clf, {}, cv=cv, \n",
    "                     scoring=scorer, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_local,y_local)\n",
    "    from dask_ml.wrappers import ParallelPostFit\n",
    "    grid_search_parallel_predictor = \\\n",
    "        ParallelPostFit(grid_search)\n",
    "    y_pred = grid_search_parallel_predictor.predict(X)\n",
    "    clf = grid_search.best_estimator_\n",
    "    score = grid_search.best_score_\n",
    "    print(f\"Score: {score}\")\n",
    "    \n",
    "    # Save model\n",
    "    if (0.85 < score) & (score < 0.998):\n",
    "        param_set_suffix = ''.join([f'__{param}_{val}' for param, val \n",
    "                                    in param_set.items()])\n",
    "        model_dir = '../models'\n",
    "        Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
    "        dump(clf, f'{model_dir}/hazard_classifier_{score:0.4f}{param_set_suffix}.joblib')\n",
    "    \n",
    "        # Visualize predictions\n",
    "        y_pred_vis = xr.DataArray(data=y_pred.reshape(output_shape), \n",
    "                                  coords=output_coords, dims=output_dims)\n",
    "        fig = plt.figure(figsize=(12,7))\n",
    "        y_pred_vis.plot.imshow(vmin=0, vmax=3)\n",
    "        plt.title('Output Classifications (WOfS + S2 MNDWI, AWEI + SRTM DEM + CHIRPS)\\n' + \\\n",
    "                  ''.join([f'{param}: {val}, ' for param, val in param_set.items()]) + \\\n",
    "                  f' CV Score: {grid_search.best_score_:.2%}')\n",
    "        vis_dir = '../outputs'\n",
    "        Path(vis_dir).mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(f'{vis_dir}/hazard_{score:0.4f}{param_set_suffix}.png')\n",
    "        plt.close(fig)\n",
    "    \n",
    "    print(f\"{(i+1)/len(param_sets_vals):.2%} through parameter sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify the areas and time range**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "areas = OrderedDict(\n",
    "    [\n",
    "        ('Area1', ((15.8193, 16.4064), (-16.5475, -15.7261))),\n",
    "        ('Area2', ((14.9893, 15.8185), (-17.1027, -16.4953))),\n",
    "        # Dakar\n",
    "        ('Area3', ((14.4290, 15.0000), (-17.5581, -17.0007))),\n",
    "        # Delta du Saloum W\n",
    "        ('Area4', ((13.5361, 14.3098), (-17.0092, -16.3564))),\n",
    "        # Delta du Saloum E\n",
    "        ('Area5', ((13.5361, 14.3098), (-16.3564, -15.9413))),\n",
    "        ('Area6', ((13.0470, 13.5400), (-16.8582, -15.5425)))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for area_ind, (area_name, (lat, lon)) in enumerate(areas.items()):\n",
    "    \n",
    "    common_load_params = \\\n",
    "    dict(output_crs=\"EPSG:4326\",\n",
    "         resolution=(-0.00027,0.00027),\n",
    "         latitude=lat, longitude=lon,\n",
    "         dask_chunks={'time':40, \n",
    "                      'latitude':1000, \n",
    "                      'longitude':1000})\n",
    "    \n",
    "    ## Load geospatial data ##\n",
    "\n",
    "    ### Load Sentinel-2 ###\n",
    "    s2_data = []\n",
    "    for time_range in time_ranges:\n",
    "        try:\n",
    "            data = load_ard(dc, products=['s2_l2a'],\n",
    "                            measurements=[\n",
    "                                # Used by MNDWI, AWEI_ns, AWEI_sh\n",
    "                                'green', 'swir_1', \n",
    "                                # Used by AWEI_ns, AWEI_sh\n",
    "                                'nir', 'swir_2',\n",
    "                                # Used by AWEI_sh\n",
    "                                'blue',\n",
    "                                'AOT', 'SCL'], \n",
    "                            time=time_range,\n",
    "                            **common_load_params)\n",
    "            data = calculate_indices(data, index='MNDWI', collection='s2')\n",
    "            data = calculate_indices(data, index='AWEI_ns', collection='s2')\n",
    "            data = calculate_indices(data, index='AWEI_sh', collection='s2')\n",
    "            data = data[['MNDWI', 'AWEI_ns', 'AWEI_sh', 'SCL', 'AOT']]\n",
    "            s2_data.append(data)\n",
    "        except:\n",
    "            continue\n",
    "    s2_data = xr.concat(s2_data, dim='time')\n",
    "    \n",
    "    # Rename data variables to distinguish them from those of other datasets when we merge\n",
    "    s2_data = s2_data.rename({data_var: f\"s2_{data_var}\" for data_var in s2_data.data_vars})\n",
    "\n",
    "    ### End Load Sentinel-2 ###\n",
    "\n",
    "    ### Sentinel-2 Stats ###\n",
    "\n",
    "    # Calculate mean of bare soil, water, and bare soil to water transitions across time\n",
    "    s2_bare_soil = s2_data.s2_SCL == 5\n",
    "    s2_water = s2_data.s2_SCL == 6\n",
    "    s2_bare_soil_mean = s2_bare_soil.mean('time')\n",
    "    s2_water_mean = s2_water.mean('time')\n",
    "    s2_soil_to_water = s2_water.isel(time=slice(1, len(s2_data.time))) & \\\n",
    "                       s2_bare_soil.isel(time=slice(0, len(s2_data.time)-1)).data   \n",
    "    s2_soil_to_water_mean = s2_soil_to_water.mean('time')\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    from scipy.stats import mode\n",
    "    s2_stats = [{\n",
    "                 f'{data_var}_min':   s2_data[data_var].min('time'), \n",
    "                 f'{data_var}_mean':  s2_data[data_var].mean('time'), \n",
    "                 f'{data_var}_std':   s2_data[data_var].std('time'), \n",
    "                 f'{data_var}_max':   s2_data[data_var].max('time')\n",
    "                }\n",
    "                for data_var in s2_data.data_vars if data_var != 's2_SCL'] + \\\n",
    "               [{\n",
    "                 # The most common classification for each pixel.\n",
    "#                  's2_SCL_mode': xr.DataArray(mode(s2_data.s2_SCL, axis=s2_data.s2_SCL.get_axis_num('time'))[0].squeeze(), \n",
    "#                  coords={'latitude':s2_data.latitude, 'longitude':s2_data.longitude}, \n",
    "#                  dims=['latitude', 'longitude']),\n",
    "                 's2_soil_to_water_mean': s2_soil_to_water_mean, \n",
    "                 's2_bare_soil_mean': s2_bare_soil_mean,\n",
    "                 's2_water_mean': s2_water_mean,\n",
    "                }]\n",
    "\n",
    "    s2_stats = xr.Dataset(dict(ChainMap(*s2_stats)))\n",
    "    \n",
    "    # Impute remaining NaNs with the means\n",
    "    for data_var in s2_stats.data_vars:\n",
    "        s2_stats[data_var] = s2_stats[data_var]\\\n",
    "            .where(~np.isnan(s2_stats[data_var]), s2_stats[data_var].mean())\n",
    "\n",
    "    ### End Sentinel-2 Stats ###\n",
    "\n",
    "    ### Load Elevation Data (SRTM) ###\n",
    "\n",
    "    # Only 1 time, so we remove it with `.isel(time=0)`.\n",
    "    srtm_data = \\\n",
    "        dc.load(product='srtm', \n",
    "                **common_load_params).elevation.isel(time=0)\n",
    "    # Remove no_data values.\n",
    "    srtm_data = srtm_data.where(srtm_data!=-32768)\n",
    "    # Impute remaining NaNs with the means\n",
    "    srtm_data = srtm_data.where(~np.isnan(srtm_data), srtm_data.mean())\n",
    "    \n",
    "    ### End Load Elevation Data (SRTM) ###\n",
    "\n",
    "    ### Load Precipitation Data from CHIRPS ###\n",
    "    import itertools\n",
    "    chirps_months = map(lambda month_str: month_str.zfill(2), map(str, range(1, 13)))\n",
    "    chirps_data = xr.concat([xr.open_rasterio(f'../precipitation/chirps/chirps-v2.0.{year}.{month_str}.tif').squeeze()\n",
    "                             .sel(y=slice(*lat[::-1]), x=slice(*lon)) \n",
    "                             for year, month_str in itertools.product(years, chirps_months)], \n",
    "                            dim='time')\n",
    "    \n",
    "    # Set missing data points to NaN\n",
    "    chirps_data = chirps_data.where(chirps_data != -9999).rename({'x': 'longitude', 'y':'latitude'})\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    chirps_stats = [{\n",
    "             f'chirps_min':   chirps_data.min('time'), \n",
    "             f'chirps_mean':  chirps_data.mean('time'), \n",
    "             f'chirps_std':   chirps_data.std('time'), \n",
    "             f'chirps_max':   chirps_data.max('time')\n",
    "            }]\n",
    "    chirps_stats = xr.Dataset(dict(ChainMap(*chirps_stats)))\n",
    "    \n",
    "    # Impute remaining NaNs with the means\n",
    "    for data_var in chirps_stats.data_vars:\n",
    "        chirps_stats[data_var] = chirps_stats[data_var]\\\n",
    "            .where(~np.isnan(chirps_stats[data_var]), chirps_stats[data_var].mean())\n",
    "    \n",
    "    # Fit the CHIRPS data to the Landsat and Sentinel-2 data\n",
    "    chirps_stats = chirps_stats.reindex(latitude=s2_stats.latitude, longitude=s2_stats.longitude, method='nearest')    \n",
    "    \n",
    "    ### End Load Precipitation Data from CHIRPS ###\n",
    "    \n",
    "    ## End Load geospatial data ##\n",
    "    \n",
    "    ## Combine datasets ##\n",
    "    \n",
    "    print(\"\\nCalculating stats...\")\n",
    "    merged_stats = xr.merge((s2_stats, srtm_data, chirps_stats)).persist()\n",
    "    output_shape = merged_stats.s2_MNDWI_mean.shape\n",
    "    output_coords = merged_stats.coords\n",
    "    output_dims = merged_stats.dims\n",
    "\n",
    "    ## End Combine datasets (L8, S2, SRTM) ##\n",
    "\n",
    "    ## Format the Feature Matrix ##\n",
    "    print(\"\\nFormatting data for model...\\n\")\n",
    "    X = merged_stats.to_array().transpose('latitude', 'longitude', 'variable')\n",
    "    X = X.stack(row=('latitude', 'longitude')).transpose('row', 'variable').persist()\n",
    "    # Clear the persisted data in `merged_stats` to save memory.\n",
    "    del merged_stats\n",
    "\n",
    "    ### Remove unneeded data ###\n",
    "\n",
    "    # Clear the persisted data in `merged_stats` to save memory.\n",
    "    if 'merged_stats' in globals():\n",
    "        del merged_stats\n",
    "        \n",
    "    ### End Remove unneeded data ###\n",
    "    \n",
    "    ## End Format the Feature Matrix ##\n",
    "    \n",
    "    ## Create and save a map for each classifier ##\n",
    "    ## defined by the parameter grid ##\n",
    "    \n",
    "    param_set_keys = list(param_grid.keys())\n",
    "    param_sets_vals = list(itertools.product(*param_grid.values()))\n",
    "    for i, param_set_vals in enumerate(param_sets_vals):\n",
    "        param_set = {k:v for k, v in zip(param_set_keys, param_set_vals)}\n",
    "        param_set_suffix = ''.join([f'__{param}_{val}' for param, val \n",
    "                                    in param_set.items()])\n",
    "        \n",
    "        # Load the model if it exists.\n",
    "        import glob\n",
    "        clf_filepath = None\n",
    "        for path in glob.glob(f'../models/hazard_classifier_*{param_set_suffix}.joblib'):\n",
    "            clf_filepath = path\n",
    "        if clf_filepath is None:\n",
    "            continue\n",
    "        if os.path.exists(clf_filepath):\n",
    "            from dask_ml.wrappers import ParallelPostFit\n",
    "            clf = ParallelPostFit(load(clf_filepath))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        print(f\"Creating hazard map for param set {param_set}.\")\n",
    "            \n",
    "        # Get predictions.\n",
    "        y_pred = clf.predict(X)\n",
    "        \n",
    "        # Get the score from the model filename.\n",
    "        import re\n",
    "        score = float(re.compile(\".*classifier_(.*?)__.*\").search(clf_filepath).group(1))\n",
    "        \n",
    "        # Save output (flood hazard map)\n",
    "        y_pred_vis = xr.DataArray(data=y_pred.reshape(output_shape), \n",
    "                                  coords=output_coords, dims=output_dims)\n",
    "        fig = plt.figure(figsize=(12,7))\n",
    "        y_pred_vis.plot.imshow(vmin=0, vmax=3)\n",
    "        plt.title('Flood Hazard Map (WOfS + S2 MNDWI, AWEI + SRTM DEM + CHIRPS)\\n' + \\\n",
    "                  ', '.join([f'{param}: {val}' for param, val in param_set.items()]))\n",
    "        vis_dir = f'../hazard_maps/{area_name}'\n",
    "        Path(vis_dir).mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(f'{vis_dir}/{score:0.4f}{param_set_suffix}.png')\n",
    "        fig.clf()\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(f\"{(i+1)/len(param_sets_vals):.2%} through parameter sets.\")\n",
    "\n",
    "    ## End Create and save a map for each classifier ##\n",
    "    ## defined by the parameter grid ##\n",
    "    print(f\"\\n{(area_ind+1)/len(areas):.2%} through areas.\")\n",
    "    print()\n",
    "    client.restart() # Clear Dask memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
